web cache (proxy server)
	
	프록시 서버는 웹 상에서 캐시의 역할을 한다.
	원래는 클라이언트의 request를 서버에서 직접 처리했지만, 서버의 직접적인 관여 없이 클라이언트의
	request를 처리해주는 하드웨어 + 소프트웨어 기술이 웹 캐시(프록시 서버)의 목표이다.
	
	웹 브라우저에서 프록시로 설정을 바꾸게 되면 HTTP에 대한 연결은 무조건 프록시 서버를 통해서 이루어지게
	바뀌게 된다.  클라이언트는 프록시에게 요청을 위탁을 하게 되고, 프록시는 중간에 이 요청을 받아
	실제 서버로 대행요청을 하게 된다. 그렇다면 서버는 요청을 보낸 호스트의 정보를 클라이언트로 보는
	것이 아니라 중간에 거친 프록시로 인지하게 되고, 다시 이 프록시에게 응답을 보낸다. 그러면 프록시는
	서버의 응답을 받고, 다시 원래의 클라이언트에게 응답을 보낸다. 즉 양방향으로 완전히 프록시를 거친는 양상을 보인다.
	
	여기까지는 프록시의 장점이 미미해보이지만, 프록시는 "캐싱"의 역할을 한다고 했다.
	즉 앞의 과정에서 프록시는 응답의 Content를 저장해두고, 다른 client에게서 같은 요청이 온다면
	서버로 요청을 보내지 않고 바로 클라이언트에게 저장해둔 응답을 보내게 된다.
	
	캐시 히트, 캐시 미스 짚고가자. 캐시가 있어서 캐시의 값을 바로 쓴다면 캐시 히트, 그렇지 않다면 캐시 미스이다.
	만약 서버가 처음 열고 여러명의 사용자가 그 서버에 접속을 하고싶어한다면, 가장 처음 사용자가 
	캐시 미스를 겪고 나머지 사용자는 같은 요청에 대해서 가까운 프록시 서버에서 캐시 히트를 통해 빠르게 접속 가능하다.
	
	프록시 서버는 서버와 클라이언트의 역할을 같이 한다. 클라이언트에게는 서버의 역할을, 서버에게는 
	클라이언트의 역할을 함.
	
	그리고 프록시는 access link의 트래픽을 엄청나게 줄일 수 있다.
	만약 프록시가 없다면 100명이 동시 접속한다고 할 때 access link에 100번의 connection이 생길 것이다.
	하지만 프록시가 있다면 이 링크에 1번만 접속이 발생하고 나머지는 프록시 내부적으로 처리한다.
	프록시 서버는 굉장히 넓게 퍼져있다. 그리고 이것은 인터넷의 입장에서 엄청난 메리트인데,
	앞서 말한대로 access link의 부담을 프록시가 줄여주기 때문이다. 

	/*
	Caching example 페이지에서, LAN utilization은 어떻게 구할까?
	영상에서는 access link의 용량과 LAN의 용량?을 나눈다. 1.54Mbps / 1Gbps
	만약 1이 100% 이용하는 비율이라고 하면 랜은 0.0015 사용하고 있다고 한다
	하지만 access link의 Utilization은 1.5 / 1.54 = 0.97이다.
	그리고 network intensity가 높을 수록 queueing delay가 늘어난다. */

	<추가내용>
	위 예제는 교수님이 다른 예제를 들어서, 즉 다른 data set으로 설명을 하신거라고 하고, ppt기준으로 보면 다음과 같다.
	다시 쓰자면, 랜이 총 받아들일 수 있는 capacity는 1gbps, 그리고 access link의 rate는 15Mbps이므로
	15Mbps / 1Gbps = 1.5%, 즉 LAN의 활용도는 1.5%이다.
	acces link의 활용도는 거의 100%에 가까운, 아주 바쁜 상태고 이로 인해서 queueing delay 등이 발생할 수 있는 상태라고 하겠다.
	총 딜레이는 Internet delay + Access delay + LAN delay로 계산하는데, 이 경우에선 Access delay가 엄청나게 커진다.  (분 단위)

	이렇게 되면 엄청나게 느려지게 된다. 그렇다면 이를 어떻게 해결할까?
	가장 명확한 답은 빠른 access link를 도입하는 것이다. 즉 154Mbps로 100배 늘린다면
	access link Utilization은 0.0097로 늘어나게 된다.
	하지만 이 방법은 가격이 너무 비싸기 때문에, 우리는 web cache를 이용한다.

	web cache를 쓸 경우 중복된 요청은 프록시가 처리하기 때문에 서버로 가는 request의 양이 현저히 줄어든다.
	만약 캐시 히트 비율이 0.4라면 0.6의 요청만이 서버로 보내진다.
	★average end-end delay : cache hit 와 cache miss 두개의 케이스를 모두 계산해서 평균을 내야한다.

	<추가 내용>
	만약 캐시 히트 확률이 0.4라면 우리는 0.6의 확률만을 계산하면 된다.
	15 * 0.6 (data rate with cache miss)/ 15(access link capacity) = 0.6.
	즉 거의 100%에 가까운 수치에서 60%로 줄어든다. 
	
	하지만 장점만 있는 것은 아니다. 캐시를 사용함으로서 새로운 문제가 생길 수도 있는데, 만약 첫번째 
	request 이후 서버의 contents가 바뀐 경우, 사용자들은 새로운 contents를 받지 못하고 데이터의 최신화가
	이루어지지 않게 된다. 이것을 해결하기 위해 Conditional GET 이라는 기능이 있다.
	클라이언트가 서버에게 request를 보낼 때 get이라는 comment를 쓰는데, 여기서 조건문을 쓴다.
	날짜값을 parameter로 보내고 만약 modified 되었다면 업데이트를, not-modified 되었다면 가지고 있던
	값을 계속 쓰는 것이다. 