TCP, sender 와 receiver 사이에 일어나는 reliable한 전송을 위한 것들에 대해 수요일에 퀴즈를 봄.

tcp connection management 마지막 부분 : 
	Go-Back-N 퀴즈 주로 나옴.
	duplicate ACK 기억하기
	실제 받은 ACK은 duplicate ACK + 1개
	타이머는 가장 오래된 패킷 기준 1개만.
	중첩 ACK이 3개 이상 오면 그냥 재전송 해버린다.
		이것이 TCP fast retransmit이다.

Principles of congestion control : 
	Traffic Intensity : La/R
	패킷을 전달해주는 라우터의 성능을 계산하는 식이다.
	이 값이 0에 가까우면 널널한 것이고, 1에 가까워지면 라우터가 많이 활용됨과 동시에
	순간순간 딜레이가 생길 수도 있는 상태이다.(평균값이기 때문에)

	버퍼가 꽉 찼을 때 그냥 버려버리는 기법이 drop tail이었다. 복습.
	

	Congestion : 혼잡
		★굉장히 중요한 개념이라고 한다.
		sender가 보내는 데이터들이 네트워크에서 수용할 수 있는 양보다 많아지면
		Congestion이 발생한다. 
		당연히 라우터 입장에서는 Congestion을 감지할 수 있다. 버퍼를 보면 된다.
		그리고 네트워크에서 Congestion은 피할 수 없는 문제이다.
		그렇다면 transport layer의 sender는 이것을 어떻게 다뤄야 할까
		application이 데이터를 보내라! 해서 무조건 다 나가는 것이 아니라 transport layer에서
		그것을 데이터를 관리하고 내보낸다. 데이터를 보낼 때 congestion 상태를 보고 보냄.
		

		일단 Congestion이 일어나면 어떤 비효율성이 있을까?
		lamda(in) : 단위 시간당 보내는 bit라고 생각
		A와 B가 같은 라우터(버퍼는 무한이라 가정)를 공유하고 서로 경쟁한다고 생각해보자.
		A의 receiver에서 단위시간당 lamda(in) 만큼 받으면 아주 잘 받고 있는거다.
		통상적으로 R/2까지는 수신율이 계속 증가하지만 NIC 등의 하드웨어적 제약으로
		최대 수신할 수 있는 데이터는 R/2	이다. 
		★만약 A와B가 경쟁하는 상황에서 둘 다 R/2로 보내버리면 딜레이가 엄청 발생하게 된다.
		여기까지가 시나리오 1.

		위 예제에서 버퍼의 크기가 한정적인 라우터로 같은 상황을 생각해보자.
		실제적으로 나가는 데이터의 양을 lamda(in)으로 잡아도 ACK 재전송 등을
		고려하면 lamda(in)보다 커질 수 있다. 이것을 lamda'(in)으로 표기한다.
		
		만약 버퍼가 꽉 찬 것을 sender가 바로 알아채고 데이터가 drop되지 않게 조절해서 
		보낸다면 R/2까지만 그래프가 이루어지고 직선의 형태를 이룬다.
		하지만 불가능한 일이죠?
		실제의 그래프는 후반부에서 x값에 대해서 같은 y값이 매칭되지 않고 조금 작게 매칭된다.(낭비가 발생)
		waste에 의해서 R/2부분은 x와 y의 매핑차이가 꽤 크다.  따라서 R/2를 전송율로 
		삼는 것은 좋지 못한 방법이다.
		여기까지가 시나리오 2


		시나리오 3
		A와 B사이에 있는 라우터는 A로부터는 바로 데이터를 받아들이고 D로부터는
		라우터를 하나 거쳐서 오는 데이터를 받아들인다. 즉 D로부터는 한번 통제가 된
		데이터를 받아들인다. 따라서 이 라우터는 D보다는 A의 데이터를 더 많이 받아들이게 된다.
		시나리오 3와 같은 경우에는 R/2 근방에서 전송율이 굉장히 저조하다.
		핵심은 현재의 네트워크 체계에서 최대 전송율로 보내는 것은 오히려 저조한 수신율로 이어진다는 것.
		그런데 이 전송율과 수신율의 상관관계에 대한 데이터는 sender가 알 수가 없다.
		sender는 이 정보를 모르는 상태에서 어떤 자세를 취해야 할까?
		sender는 일단 데이터를 보내보고 돌아오는 ACK(feedback)을 보고 congestion 상황을 유추해야 한다.
		

	요약 : TCP sender들은 congestion 상황을 모르고, 이 정보를 혼자 알아내서 최적의 송신율을 찾아햐 함.
	방법 : 처음 데이터를 하나 보내보고 ACK이 잘 오면 그 다음은 2개를 보내보고, 잘 오면 4개, 8개, 16개 ... 이렇게 보내본다.
	그러다 duplicate ACK이 하나 둘 생기고, 3개 이상 duplicate ACK이 발생하면 다시 줄여서 보내보고...
		(fast retransmission)
	이런 식으로 경험에 기반해서 최적의 송신율을 찾는다. 	
	이것이 Bandwidth probing :  probe : 조사하다
	그리고 이 Bandwidth를 조절하는 알고리즘이 AIMD이다.
	Additive increase/multiplicative decrease		

	그리고 이렇게 한번 loss를 경험하면 다음부터는 데이터 송신율을 조금씩 올리면서 찾는다.
	조금씩 늘리는 것은 pipelining의 window 사이즈를 조절하는 것이다.
	이것이 ★congestion control★
	underutilize : 자원을 쓸 수 있음에도 쓰지 않는 것. congestion 컨트롤에서 보내는 패킷의 수를 너무 낮게 잡으면 발생
	

	추가적으로 이렇게 ACK을 이용한, 수신자와 송신자 사이에 교신만으로 congestion을 조절하는
	기법을 end - end control이라고 한다.
	TCP는 현재 이 전략을 취하고 있다고 한다.	
	
	정리하면 sender가 receiver에 반응해서(버퍼?) 더 보낼 수 있지만 적게 보내는 것 -> flow control
	즉 수신자가 30만 받을 수 있으면 30만 보내는 것
	sender가 더 보낼 수 있지만 네트워크의 congestion 상태에 따라 적게 보내는 것 -> congestion control
	이 둘이 같이 동작을 하는데 이 둘을 어떻게 combine을 시킬까
	단순히 두 상황 모두에서 더 작은 값을 쓴다. 즉 Congestion control에 의하면 30만 보내야 되고,
	flow control에 의하면 100만 보내야 한다면 30만을 보낸다.

	new flow start에서 시작 값을 어떻게 올릴까?
	1 Gbs 인 상황에서 underutilize가 너무너무 심할 수 있는 상황 알기.
	이런 상황에서 보내는 rate을 aggresive하게 확 올려서 loss를 뻘리 겪고 예상해서 조정한다.
	이때 AIMD의 Multiplicative 즉 곱셈을 쓰고 시작할때는 특수하게 지수승, Exponential 하게 값을 증가시킨다.
	대신에 작은 size의 sending rate으로 시작하되 올리는 속도를 빠르게 올린다. 
	이것이 Slow Start.

	과정은 다음과 같다.
	처음 시작에서 지수승으로 빠르게 증가 시키다가 loss가 발생하면 반으로 확 줄이고(Multiplicative) 그 다음부터 additive하게
	하나씩 증가시킨다.
	
	
	★loss가 일어난 것은 timeout과 Triple duplicate ACK이다.
	그리고 둘 중에서 timeout이 더 상황이 심각한 경우이다. timeout은 ACK이 아예 안오는 경우이고
	Triple duplicate ACK은 ACK이 오는데 중간에 손실이 생긴 문제이기 때문이다.
	따라서 sender는 이 둘을 다르게 취급한다.

	Triple duplicate ACK은 AIMD에서 MD를 쓴다.
	Timeout은 네트워크 상태가 안좋아졌다고 판단해서 아예 다시 측정을 한다. 따라서 slow start부터 다시 시작한다. 
	하지만 이때 ssthresh 값이 있고 이 값을 넘은면 Additive하게 조금씩 증가를 한다.
	이 단계를 Congestion Avoidance(CA) 단계라고 한다.	
	만약 GBN에서 N이 4인데 CA 상황에 있다고 하면, 다음 패킷의 개수는 5이다.
	즉 패킷마다 1/N을 더해주면, 결과적으로 현재 패킷 + 1이 된다.
	이 과정에서 또 timeout이 발생하면 slow start thresh hold 값은 반으로 줄고 다시 slow start로 시작한다.

	TCP Fairness
	긴~~시간 여러개의 TCP Connection이 특정 link를 share하고 있을 때 각각의 TCP session들은 공평하게 bandwidth를 가져야한다.
	즉 k개의 tcp 연결이 있을 때 이들은 1/k로 공평하게 나눠가져야 한다.
	만약 한명이 공평성을 깨버리면 안된다. 따라서 TCP 연결을 새로 만들 때 공평성을 고려하고 만들어야 한다.
	그리고 AIMD로 TCP를 만들면 이 공평성이 Guarantee가 된다고 한다.
		
	
	그리고 부가적으로 라우터가 약간의 도움을 줄 수 있다. 버퍼가 꽉 찼다면 그 정보를 ACK을 보낼 때 같이 붙여서 보내주고,
	sender가 이 정보를 보고 duplicate ACK이 일어나지 않아도 송신율을 조절한다.
	이것이 network assistant control? 여튼 참고만 하라고 하심.	
	
	

	TCP Tahoe -> + Fast Recovery -> TCP Reno
	Tahoe 가 아주 기본이 되는 congestion control algorithm,
	여기에는 위에서 본 AIMD, SS 등이 있고, 여기에 Fast Recovery가 추가된 것이 Reno
	+ CUBIC 알고리즘 : 리눅스에 기본으로 들어가는 TCP Congestion algorithm, y = x^3의 그래프 형태로 평균을 맞추는 형태.

