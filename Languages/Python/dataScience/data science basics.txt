빅데이터와 데이터과학의 차이점
	빅데이터 : 비즈니스 특화
	데이터과학 : 머신러닝 특화
##########################################################################################

빅데이터란?
	process and techniques to discover from data answers or insights to solve a business problem

빅데이터 엔드 투 엔드 과정
	Objective setting
	Data curation
	Data Inspection
		Data exploration
			데이터의 특성(distribution, outliers, correlation), matplotlib의 여러 라이브러리들을 사용할 수 있다.
		Suitability check
			process metadata, business metadata, technical metadata (참고만)
	Data preprocessing
		Data restructuring
		Data value Changes
		Feature Engineering
		Data reduction
	Data analysis (modeling)
	Evaluation of the results
	(deployment)
Data Preprocessing
	Data restructuring
		Table decomposition
		Table merge
	Data value Changes
		Cleaning dirty data : dirty data를 여러가지 방법으로 없애기
			missing data
			wrong data
			unusable data
			outliers
		Text preprocessing : html 헤더 제거, 마침표나 느낌표 같은 구두점 처리 등
		Data discretization : 남자/여자, 20대/30대 등과 같이 경계를 나누고 여러개의 bin으로 나눔
		Data normalization / standardization
			data scaling : std(z-score), min-max, robust (얘네 다 계산할 수 있어야 함.)
		Encoding for data mining algorithms
	Feature Engineering
		feature creation
			deriving feature from existing features
				entity, agg, ...
			converting values to new features
				encoders (label, one-hot)
		feature selection
			Univariate Selection(SelectKBest)
				selectKBest는 target entity를 산출하기 위해 각 indenpendent entity가 얼마나 중요한지를
				score(fit.scores_)로 나타내어 확인할 수 있다.
			Feature importance scoring
			correlation matrix with heatmap
		feature reduction
			Principal Component Analysis
			Eigenvector(pc) : directions where there is the most variance
			즉 PC로 correlation 정도 파악해서 묶어버림.
		특정 결과 산출하는데 필요없는 값들 drop
		중요하지 않은 feature 없애기, 새로운 feature 만들기
			기존 feature로부터 새로운 feature만들기 : 
				income 값이 104564 라면 log해서 11.557555로 바꾸기(수가 작아지면 더 빨라진다.)
				날짜도 월만 필요하다면 월만 쓴다.
		LabelEncoder, OnehotEncoder ... 
	Data reduction
		Feature Selection/Reduction
		Data Filtering
		Data Summarization
		Concept Hierarchy
		Data Compression
			loss-less
			lossy
		Nemerosity Reduction
			parametric
			non-parametric(clustering)
				discretization
					equal width bins
					equal frequency bins
					entropy based
				clustering
				sampling
##########################################################################################
feature selection과 feature reduction의 차이 : 둘 다 feature의 수를 감소시키지만 
feature selection의 경우 feature를 변경하지 않고 선택해서 감소시키는 거라면 feature reduction은 feature의 형태를 바꾼다.
##########################################################################################	

데이터 전처리의 missing value 처리 : 
	isna()(.any(), .sum())
	na_values
		missing_values = ["?", "--"]
		df_test = pd.read_csv("dataset.csv", na_values = missing values)	# ?, -- 를 NA로 치환한다.
	replace
		x.replace({"?" : np.nan, "--" : np.nan}, inplace = True)
	dropna(any, all, thresh)
		df.dropna(axis=0, how='all', inplace=True)
		how의 default는 'any'이다. all의 경우 row의 모든 value가 na여야 drop한다.
	fillna
		fillna(25) : na값을 25로 채운다.
##########################################################################################
dirty data의 종류(와 발생 이유? 대처법? 너무 많은데)
	missing data
		(empty blank)
	wrong data
		height : 248
	unusable data
		dollar, $, 김대중, DJ ... 
	outliers
		outlier = value > mean ± 3 standard deviations

##########################################################################################
정규화를 왜 하는가?
	머신러닝 알고리즘은 데이터가 서로 유사한 규모이거나 정규 분포에 가까울 때 더 나은 성능을 발휘한다고 한다.
	또 평균값 등이 skewed 형태에서는 대표값으로 다뤄질 수 없지만 정규화된 form에서는 다뤄질 수 있다.
	민맥스, 스탠다드 ,로버스트 등 많은 스케일러들도 결국 이 작업을 위한 것들.
##########################################################################################
feature engineering의 feature selection은 왜 하는가?
	원하는 결과를 도출하는 feature만 남기고 나머지는 drop하는게 머신러닝 알고리즘의 성능을 향상시킬 수 있기 때문이다.
	여기에서 Univaiate selection(select K best(chi-square))과 feature importance scoring(Extra tree classifier), correlation matrix with heatmap 이 사용된다.
##########################################################################################
feature reduction에서의 PCA
	PCA : Principal Component Analysis
	PC : eigenvector(아이겐 벡터, 주어진 데이터의 분포에 분산이 가장 큰 방향을 뜻한다.)
		지방, 단백질이랑 섬유질, 비타민C같이 correlated 된 component는 서로 묶어서 PC로 만들 수 있다.
	
		from sklearn.decomposition import PCA
		pca = PCA(n_components=2)
		principalComponents = pca.fit_transform(x)
		(이렇게 하면 principalComponents 에는 무슨 값이 들어가는거지?)
		위 코드를 통하면 x의 feature는 판단된 correlation에 따라 2개(n_components)의 새로운 PC로 새롭게 만들어진다.
		최적의 n_components의 수는 elbow가 판단한다.



###########################################################################################

np.random.randint(최소값, 최대값, size=(행, 열)) : 범위 내의 정수를 생성. 

	ex : np.random.randint(0,4,size=(1,4))

np.random.randn((행,열)) : 표준정규분포(-1 ~ 1)로부터 샘플링된 난수를 생성한다. (정수로 쓰고싶으면 앞에 100을 곱해줘야함.)
	
	ex : np.random.randn(5)

np.random.normal(정규분포 범위, 정규분포 범위, (행,열)) : 정규분포로부터 샘플링된 난수를 생성한다.

	ex : np.random.normal(1.5, 1.5, (2,4))


###########################################################################################

aggregation을 하는 이유 : 
	
	머신러닝 알고리즘은 두개 이상의 엔티티를 한번에 처리할 수 없다.
	따라서 논리적으로 연결된 RDB같은 경우는 aggregation을 통해서 하나로 합쳐주어야 한다.

module 4 preprocessing 2 시험 전에 잘 봐두자.

###########################################################################################

피어슨의 r 구하는 공식 알아두기.
linear regression 기울기 절편 공식 알아두기. 설마 나오겠어 근데


dataframe 생성 : 기초중에 기초

	data = { "2020" : [11111,22222,33333,44444,55555], 
		"2021" : [66666,77777,88888,99999,00000] }
	dataFrame = pd.DataFrame(data, index = [~~~])

pca 돌리기 전에 스케일링 먼저,


Correlation describes the strength of a linear relationship between two variables
and Regression tells us how to draw the straight line described by the correlation



metadata : data of data

	technical metadata
	business metadata
	process metadata